{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Báo cáo Project\n",
    "Lớp TTNT-154016, Nhóm G07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thông tin chung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thành viên\n",
    "- Đỗ Huy Đạt 20220024\n",
    "- Đoàn Nguyễn Hải Nam 20220035\n",
    "- Lê Minh Triết 20220045\n",
    "- Đàm Hồng Thái 20183625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Đề xuất project (W2-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài toán\n",
    "Phân loại Email Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phương pháp\n",
    "Sử dụng Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân công\n",
    "- ĐH Đạt: Xây dựng Decision Tree, đánh giá kết quả\n",
    "- ĐNH Nam: Chuyển đổi dữ liệu thành vector, xây dựng Decision Tree\n",
    "- LM Triết: Xây dựng Decision Tree, đánh giá kết quả\n",
    "- ĐH Thái: Làm sạch, phân tích dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tiến độ giữa kỳ (W9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chương trình\n",
    "Trước hết, nhóm xây dựng một Decision Tree đơn giản dựa trên chỉ số Gini để tạo cây.\n",
    "Mã nguồn lưu trong [g7_decision_tree.py](g7_decision_tree.py).\n",
    "\n",
    "Mã nguồn cũng được triển khai trong cell dưới đây"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Class đại diện cho một nút trong cây quyết định\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature      # Đặc trưng sử dụng để chia dữ liệu\n",
    "        self.threshold = threshold  # Ngưỡng sử dụng để chia dữ liệu\n",
    "        self.left = left            # Con trỏ tới nút con bên trái\n",
    "        self.right = right          # Con trỏ tới nút con bên phải\n",
    "        self.value = value          # Giá trị của nút nếu là nút lá\n",
    "\n",
    "# Hàm tính chỉ số Gini để đo độ thuần nhất của nút\n",
    "def gini(y):\n",
    "    _, counts = np.unique(y, return_counts=True)  # Tìm các lớp và số lượng phần tử trong mỗi lớp\n",
    "    gini = 1.0 - sum((count / len(y)) ** 2 for count in counts)  # Tính chỉ số Gini\n",
    "    return gini\n",
    "\n",
    "class G07DecisionTree():\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = TreeNode()\n",
    "\n",
    "    # Hàm fit tree với datasets\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree_(X, y, depth=0, max_depth=self.max_depth)\n",
    "    \n",
    "    # Hàm dự đoán một tập dữ liệu\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_tree_(self.tree, x) for x in X])\n",
    "    \n",
    "    # Hàm chia dữ liệu theo đặc trưng và ngưỡng\n",
    "    def split_(self, X, y, feature, threshold):\n",
    "        left_mask = X[:, feature] <= threshold  # Mặt nạ để lấy các phần tử nhỏ hơn hoặc bằng ngưỡng\n",
    "        right_mask = X[:, feature] > threshold  # Mặt nạ để lấy các phần tử lớn hơn ngưỡng\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    # Hàm tìm đặc trưng và ngưỡng tốt nhất để chia dữ liệu\n",
    "    def best_split_(self, X, y):\n",
    "        best_gini = 1.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        for feature in range(X.shape[1]):  # Duyệt qua từng đặc trưng\n",
    "            thresholds = np.unique(X[:, feature])  # Tìm tất cả các ngưỡng duy nhất\n",
    "            for threshold in thresholds:  # Duyệt qua từng ngưỡng\n",
    "                X_left, X_right, y_left, y_right = self.split_(X, y, feature, threshold)  # Chia dữ liệu\n",
    "                if len(y_left) == 0 or len(y_right) == 0:  # Nếu một trong hai phần trống, bỏ qua\n",
    "                    continue\n",
    "                gini_left = gini(y_left)  # Tính chỉ số Gini cho phần bên trái\n",
    "                gini_right = gini(y_right)  # Tính chỉ số Gini cho phần bên phải\n",
    "                gini_split = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)  # Tính chỉ số Gini trung bình\n",
    "                if gini_split < best_gini:  # Nếu Gini nhỏ hơn, cập nhật đặc trưng và ngưỡng tốt nhất\n",
    "                    best_gini = gini_split\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # Hàm xây dựng cây quyết định đệ quy\n",
    "    def build_tree_(self, X, y, depth=0, max_depth=10):\n",
    "        if len(np.unique(y)) == 1:  # Nếu tất cả các phần tử cùng một lớp, trả về nút lá\n",
    "            return TreeNode(value=y[0])\n",
    "        if depth >= max_depth:  # Nếu độ sâu đạt giới hạn, trả về nút lá\n",
    "            return TreeNode(value=np.bincount(y).argmax())  # Trả về lớp phổ biến nhất\n",
    "        feature, threshold = self.best_split_(X, y)  # Tìm đặc trưng và ngưỡng tốt nhất\n",
    "        if feature is None:  # Nếu không tìm được đặc trưng tốt, trả về nút lá\n",
    "            return TreeNode(value=np.bincount(y).argmax())  # Trả về lớp phổ biến nhất\n",
    "        X_left, X_right, y_left, y_right = self.split_(X, y, feature, threshold)  # Chia dữ liệu\n",
    "        left_child = self.build_tree_(X_left, y_left, depth + 1, max_depth)  # Xây dựng nút con bên trái\n",
    "        right_child = self.build_tree_(X_right, y_right, depth + 1, max_depth)  # Xây dựng nút con bên phải\n",
    "        return TreeNode(feature=feature, threshold=threshold, left=left_child, right=right_child)\n",
    "    \n",
    "    # Hàm dự đoán giá trị dựa trên cây quyết định\n",
    "    def predict_tree_(self, node, X):\n",
    "        if node.value is not None:  # Nếu là nút lá, trả về giá trị của nút lá\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:  # Nếu giá trị nhỏ hơn hoặc bằng ngưỡng, duyệt cây con bên trái\n",
    "            return self.predict_tree_(node.left, X)\n",
    "        else:  # Nếu giá trị lớn hơn ngưỡng, duyệt cây con bên phải\n",
    "            return self.predict_tree_(node.right, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đây là kết quả chạy thử với bộ dữ liệu đầu vào đã được mã hóa TF-IDF sang vector (chi tiết sẽ được báo cáo đầy đủ sau) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('tfidf.csv')\n",
    "y = pd.read_csv('tfidf_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.head(200).to_numpy()\n",
    "y_train = y.head(200).to_numpy()[:,0]\n",
    "X_test = X.tail(200).to_numpy()\n",
    "y_test = y.tail(200).to_numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 57s\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtree = G07DecisionTree(max_depth=6)\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dtree.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết quả, vấn đề gặp phải\n",
    "Như vậy, nhóm G07 đã thử nghiệm Decision Tree đã xây dựng và đạt độ chính xác 89% khi mới chỉ huấn luyện trên 200 hàng đầu của datasets.\n",
    "\n",
    "Tuy vậy, hạn chế vẫn còn khi huấn luyện 200 hàng đầu này đã mất khoảng 10 phút (với 3 phút 27 s CPU). Trong những tuần sau nhóm sẽ tập trung vào cải thiện thời gian fit cho datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cập nhật kết quả cuối kỳ (W15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xử lý dữ liệu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tải dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu được lấy từ nguồn sau: [Spam Email Dataset](https://www.kaggle.com/datasets/venky73/spam-mails-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.17 / client 1.6.12)\n",
      "Dataset URL: https://www.kaggle.com/datasets/venky73/spam-mails-dataset\n",
      "License(s): CC0-1.0\n",
      "spam-mails-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download venky73/spam-mails-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('spam-mails-dataset.zip') as f:\n",
    "    f.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv('data/spam_ham_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     0.710114\n",
       "spam    0.289886\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuL0lEQVR4nO3df1TUdaL/8RegjCAOhgIDK5plqSRYkVfnlK6lOSq5/bA20+uP1SwN6ypd5bLH9edutJqZmulWt7C9Wlmb/ZBSERdNRS12yV9p6uLBPTrg1WAUFVTm+8d+/dwm7RcBM/J+Ps6Zc/h8Pu/5zPvtORPPZj4zBHm9Xq8AAAAMFuzvCQAAAPgbQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4zXx9wSuBjU1NTp69KhatGihoKAgf08HAAD8CF6vV6dOnVJ8fLyCg7//NSCC6Ec4evSoEhIS/D0NAABQC0eOHFGbNm2+dwxB9CO0aNFC0r/+Qe12u59nAwAAfgyPx6OEhATr9/j3IYh+hEtvk9ntdoIIAICrzI+53IWLqgEAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGK+JvyeA/5My+Q1/TwEISIVzR/h7CgAaOV4hAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGM+vQbRkyRIlJyfLbrfLbrfL6XTqk08+sY737t1bQUFBPrdx48b5nKOkpESpqakKDw9XTEyMJk+erAsXLviMyc/P16233iqbzaYOHTooOzu7IZYHAACuEn79a/dt2rTRs88+qxtuuEFer1fLli3Tvffeq7///e+66aabJEljx47VrFmzrPuEh4dbP1+8eFGpqalyOBzaunWrjh07phEjRqhp06Z65plnJEnFxcVKTU3VuHHjtHz5cuXl5enRRx9VXFycXC5Xwy4YAAAEJL8G0aBBg3y2//CHP2jJkiXatm2bFUTh4eFyOBxXvP+6deu0d+9erV+/XrGxsbr55ps1e/ZsZWRkaMaMGQoNDdXSpUvVvn17zZs3T5LUuXNnbd68WfPnz//OIKqqqlJVVZW17fF46mK5AAAgQAXMNUQXL17UW2+9pcrKSjmdTmv/8uXL1bp1a3Xp0kWZmZk6c+aMdaygoEBJSUmKjY219rlcLnk8Hu3Zs8ca07dvX5/HcrlcKigo+M65ZGVlKTIy0rolJCTU1TIBAEAA8usrRJK0a9cuOZ1OnTt3ThEREVq1apUSExMlSUOHDlW7du0UHx+vnTt3KiMjQ/v379d7770nSXK73T4xJMnadrvd3zvG4/Ho7NmzCgsLu2xOmZmZSk9Pt7Y9Hg9RBABAI+b3IOrYsaOKiopUUVGhd999VyNHjtTGjRuVmJioxx57zBqXlJSkuLg49enTR4cOHdL1119fb3Oy2Wyy2Wz1dn4AABBY/P6WWWhoqDp06KCUlBRlZWWpa9euWrBgwRXHdu/eXZJ08OBBSZLD4VBpaanPmEvbl647+q4xdrv9iq8OAQAA8/g9iL6tpqbG54LmbyoqKpIkxcXFSZKcTqd27dqlsrIya0xubq7sdrv1tpvT6VReXp7PeXJzc32uUwIAAGbz61tmmZmZGjBggNq2batTp05pxYoVys/P19q1a3Xo0CGtWLFCAwcOVKtWrbRz505NmjRJvXr1UnJysiSpX79+SkxM1PDhwzVnzhy53W5NnTpVaWlp1lte48aN04svvqgpU6Zo9OjR2rBhg1auXKmcnBx/Lh0AAAQQvwZRWVmZRowYoWPHjikyMlLJyclau3at7r77bh05ckTr16/XCy+8oMrKSiUkJGjw4MGaOnWqdf+QkBCtXr1a48ePl9PpVPPmzTVy5Eif7y1q3769cnJyNGnSJC1YsEBt2rTRq6++yncQAQAAS5DX6/X6exKBzuPxKDIyUhUVFbLb7fX2OCmT36i3cwNXs8K5I/w9BQBXoZ/y+zvgriECAABoaAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeH4NoiVLlig5OVl2u112u11Op1OffPKJdfzcuXNKS0tTq1atFBERocGDB6u0tNTnHCUlJUpNTVV4eLhiYmI0efJkXbhwwWdMfn6+br31VtlsNnXo0EHZ2dkNsTwAAHCV8GsQtWnTRs8++6wKCwv1+eef66677tK9996rPXv2SJImTZqkjz76SO+88442btyoo0eP6oEHHrDuf/HiRaWmpqq6ulpbt27VsmXLlJ2drWnTplljiouLlZqaqjvvvFNFRUWaOHGiHn30Ua1du7bB1wsAAAJTkNfr9fp7Et8UFRWluXPn6sEHH1R0dLRWrFihBx98UJK0b98+de7cWQUFBerRo4c++eQT3XPPPTp69KhiY2MlSUuXLlVGRoaOHz+u0NBQZWRkKCcnR7t377YeY8iQISovL9eaNWuuOIeqqipVVVVZ2x6PRwkJCaqoqJDdbq+3tadMfqPezg1czQrnjvD3FABchTwejyIjI3/U7++AuYbo4sWLeuutt1RZWSmn06nCwkKdP39effv2tcZ06tRJbdu2VUFBgSSpoKBASUlJVgxJksvlksfjsV5lKigo8DnHpTGXznElWVlZioyMtG4JCQl1uVQAABBg/B5Eu3btUkREhGw2m8aNG6dVq1YpMTFRbrdboaGhatmypc/42NhYud1uSZLb7faJoUvHLx37vjEej0dnz5694pwyMzNVUVFh3Y4cOVIXSwUAAAGqib8n0LFjRxUVFamiokLvvvuuRo4cqY0bN/p1TjabTTabza9zAAAADcfvQRQaGqoOHTpIklJSUvTZZ59pwYIFevjhh1VdXa3y8nKfV4lKS0vlcDgkSQ6HQzt27PA536VPoX1zzLc/mVZaWiq73a6wsLD6WhYAALiK+P0ts2+rqalRVVWVUlJS1LRpU+Xl5VnH9u/fr5KSEjmdTkmS0+nUrl27VFZWZo3Jzc2V3W5XYmKiNeab57g05tI5AAAA/PoKUWZmpgYMGKC2bdvq1KlTWrFihfLz87V27VpFRkZqzJgxSk9PV1RUlOx2u5588kk5nU716NFDktSvXz8lJiZq+PDhmjNnjtxut6ZOnaq0tDTrLa9x48bpxRdf1JQpUzR69Ght2LBBK1euVE5Ojj+XDgAAAohfg6isrEwjRozQsWPHFBkZqeTkZK1du1Z33323JGn+/PkKDg7W4MGDVVVVJZfLpZdeesm6f0hIiFavXq3x48fL6XSqefPmGjlypGbNmmWNad++vXJycjRp0iQtWLBAbdq00auvviqXy9Xg6wUAAIEp4L6HKBD9lO8x+Dn4HiLgyvgeIgC1cVV+DxEAAIC/EEQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACM59cgysrKUrdu3dSiRQvFxMTovvvu0/79+33G9O7dW0FBQT63cePG+YwpKSlRamqqwsPDFRMTo8mTJ+vChQs+Y/Lz83XrrbfKZrOpQ4cOys7Oru/lAQCAq4Rfg2jjxo1KS0vTtm3blJubq/Pnz6tfv36qrKz0GTd27FgdO3bMus2ZM8c6dvHiRaWmpqq6ulpbt27VsmXLlJ2drWnTplljiouLlZqaqjvvvFNFRUWaOHGiHn30Ua1du7bB1goAAAJXE38++Jo1a3y2s7OzFRMTo8LCQvXq1cvaHx4eLofDccVzrFu3Tnv37tX69esVGxurm2++WbNnz1ZGRoZmzJih0NBQLV26VO3bt9e8efMkSZ07d9bmzZs1f/58uVyu+lsgAAC4KgTUNUQVFRWSpKioKJ/9y5cvV+vWrdWlSxdlZmbqzJkz1rGCggIlJSUpNjbW2udyueTxeLRnzx5rTN++fX3O6XK5VFBQcMV5VFVVyePx+NwAAEDj5ddXiL6ppqZGEydO1O23364uXbpY+4cOHap27dopPj5eO3fuVEZGhvbv36/33ntPkuR2u31iSJK17Xa7v3eMx+PR2bNnFRYW5nMsKytLM2fOrPM1AgCAwBQwQZSWlqbdu3dr8+bNPvsfe+wx6+ekpCTFxcWpT58+OnTokK6//vp6mUtmZqbS09OtbY/Ho4SEhHp5LAAA4H8B8ZbZhAkTtHr1av31r39VmzZtvnds9+7dJUkHDx6UJDkcDpWWlvqMubR96bqj7xpjt9sve3VIkmw2m+x2u88NAAA0Xn4NIq/XqwkTJmjVqlXasGGD2rdv/4P3KSoqkiTFxcVJkpxOp3bt2qWysjJrTG5urux2uxITE60xeXl5PufJzc2V0+mso5UAAICrmV+DKC0tTf/zP/+jFStWqEWLFnK73XK73Tp79qwk6dChQ5o9e7YKCwt1+PBhffjhhxoxYoR69eql5ORkSVK/fv2UmJio4cOH64svvtDatWs1depUpaWlyWazSZLGjRunf/zjH5oyZYr27dunl156SStXrtSkSZP8tnYAABA4/BpES5YsUUVFhXr37q24uDjr9vbbb0uSQkNDtX79evXr10+dOnXS008/rcGDB+ujjz6yzhESEqLVq1crJCRETqdT//7v/64RI0Zo1qxZ1pj27dsrJydHubm56tq1q+bNm6dXX32Vj9wDAABJUpDX6/X6exKBzuPxKDIyUhUVFfV6PVHK5Dfq7dzA1axw7gh/TwHAVein/P4OiIuqAQAA/IkgAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxahVEd911l8rLyy/b7/F4dNddd/3cOQEAADSoWgVRfn6+qqurL9t/7tw5ffrppz97UgAAAA2pyU8ZvHPnTuvnvXv3yu12W9sXL17UmjVr9Itf/KLuZgcAANAAflIQ3XzzzQoKClJQUNAV3xoLCwvTokWL6mxyAAAADeEnBVFxcbG8Xq+uu+467dixQ9HR0dax0NBQxcTEKCQkpM4nCQAAUJ9+UhC1a9dOklRTU1MvkwEAAPCHWn/s/sCBA3r55Zf1+9//XrNmzfK5/VhZWVnq1q2bWrRooZiYGN13333av3+/z5hz584pLS1NrVq1UkREhAYPHqzS0lKfMSUlJUpNTVV4eLhiYmI0efJkXbhwwWdMfn6+br31VtlsNnXo0EHZ2dm1XToAAGhkftIrRJe88sorGj9+vFq3bi2Hw6GgoCDrWFBQkKZNm/ajzrNx40alpaWpW7duunDhgn7729+qX79+2rt3r5o3by5JmjRpknJycvTOO+8oMjJSEyZM0AMPPKAtW7ZI+tfF3KmpqXI4HNq6dauOHTumESNGqGnTpnrmmWck/eutvtTUVI0bN07Lly9XXl6eHn30UcXFxcnlctXmnwAAADQiQV6v1/tT79SuXTs98cQTysjIqNPJHD9+XDExMdq4caN69eqliooKRUdHa8WKFXrwwQclSfv27VPnzp1VUFCgHj166JNPPtE999yjo0ePKjY2VpK0dOlSZWRk6Pjx4woNDVVGRoZycnK0e/du67GGDBmi8vJyrVmz5gfn5fF4FBkZqYqKCtnt9jpd8zelTH6j3s4NXM0K547w9xQAXIV+yu/vWr1l9vXXX+uhhx6q1eS+T0VFhSQpKipKklRYWKjz58+rb9++1phOnTqpbdu2KigokCQVFBQoKSnJiiFJcrlc8ng82rNnjzXmm+e4NObSOb6tqqpKHo/H5wYAABqvWgXRQw89pHXr1tXpRGpqajRx4kTdfvvt6tKliyTJ7XYrNDRULVu29BkbGxtrfQeS2+32iaFLxy8d+74xHo9HZ8+evWwuWVlZioyMtG4JCQl1skYAABCYanUNUYcOHfS73/1O27ZtU1JSkpo2bepz/KmnnvrJ50xLS9Pu3bu1efPm2kypTmVmZio9Pd3a9ng8RBEAAI1YrYLo5ZdfVkREhDZu3KiNGzf6HAsKCvrJQTRhwgStXr1amzZtUps2baz9DodD1dXVKi8v93mVqLS0VA6HwxqzY8cOn/Nd+hTaN8d8+5NppaWlstvtCgsLu2w+NptNNpvtJ60BAABcvWoVRMXFxXXy4F6vV08++aRWrVql/Px8tW/f3ud4SkqKmjZtqry8PA0ePFiStH//fpWUlMjpdEqSnE6n/vCHP6isrEwxMTGSpNzcXNntdiUmJlpjPv74Y59z5+bmWucAAABmq1UQ1ZW0tDStWLFCH3zwgVq0aGFd8xMZGamwsDBFRkZqzJgxSk9PV1RUlOx2u5588kk5nU716NFDktSvXz8lJiZq+PDhmjNnjtxut6ZOnaq0tDTrVZ5x48bpxRdf1JQpUzR69Ght2LBBK1euVE5Ojt/WDgAAAketgmj06NHfe/y11177UedZsmSJJKl3794++19//XWNGjVKkjR//nwFBwdr8ODBqqqqksvl0ksvvWSNDQkJ0erVqzV+/Hg5nU41b95cI0eO9PmCyPbt2ysnJ0eTJk3SggUL1KZNG7366qt8BxEAAJBUy+8huv/++322z58/r927d6u8vFx33XWX3nvvvTqbYCDge4gA/+J7iADUxk/5/V2rV4hWrVp12b6amhqNHz9e119/fW1OCQAA4De1/ltml50oOFjp6emaP39+XZ0SAACgQdRZEEnSoUOHLvujqgAAAIGuVm+ZffNLC6V/fXz+2LFjysnJ0ciRI+tkYgAAAA2lVkH097//3Wc7ODhY0dHRmjdv3g9+Ag0AACDQ1CqI/vrXv9b1PAAAAPzmZ30x4/Hjx7V//35JUseOHRUdHV0nkwIAAGhItbqourKyUqNHj1ZcXJx69eqlXr16KT4+XmPGjNGZM2fqeo4AAAD1qlZBlJ6ero0bN+qjjz5SeXm5ysvL9cEHH2jjxo16+umn63qOAAAA9apWb5n95S9/0bvvvuvzJzcGDhyosLAw/frXv7b+JAcAAMDVoFavEJ05c0axsbGX7Y+JieEtMwAAcNWpVRA5nU5Nnz5d586ds/adPXtWM2fOlNPprLPJAQAANIRavWX2wgsvqH///mrTpo26du0qSfriiy9ks9m0bt26Op0gAABAfatVECUlJenAgQNavny59u3bJ0l65JFHNGzYMIWFhdXpBAEAAOpbrYIoKytLsbGxGjt2rM/+1157TcePH1dGRkadTA4AAKAh1Ooaoj/96U/q1KnTZftvuukmLV269GdPCgAAoCHVKojcbrfi4uIu2x8dHa1jx4797EkBAAA0pFoFUUJCgrZs2XLZ/i1btig+Pv5nTwoAAKAh1eoaorFjx2rixIk6f/687rrrLklSXl6epkyZwjdVAwCAq06tgmjy5Mk6ceKEnnjiCVVXV0uSmjVrpoyMDGVmZtbpBAEAAOpbrYIoKChIf/zjH/W73/1OX375pcLCwnTDDTfIZrPV9fwAAADqXa2C6JKIiAh169atruYCAADgF7W6qBoAAKAxIYgAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8vwbRpk2bNGjQIMXHxysoKEjvv/++z/FRo0YpKCjI59a/f3+fMSdPntSwYcNkt9vVsmVLjRkzRqdPn/YZs3PnTvXs2VPNmjVTQkKC5syZU99LAwAAVxG/BlFlZaW6du2qxYsXf+eY/v3769ixY9btzTff9Dk+bNgw7dmzR7m5uVq9erU2bdqkxx57zDru8XjUr18/tWvXToWFhZo7d65mzJihl19+ud7WBQAAri5N/PngAwYM0IABA753jM1mk8PhuOKxL7/8UmvWrNFnn32m2267TZK0aNEiDRw4UM8995zi4+O1fPlyVVdX67XXXlNoaKhuuukmFRUV6fnnn/cJJwAAYK6Av4YoPz9fMTEx6tixo8aPH68TJ05YxwoKCtSyZUsrhiSpb9++Cg4O1vbt260xvXr1UmhoqDXG5XJp//79+vrrr6/4mFVVVfJ4PD43AADQePn1FaIf0r9/fz3wwANq3769Dh06pN/+9rcaMGCACgoKFBISIrfbrZiYGJ/7NGnSRFFRUXK73ZIkt9ut9u3b+4yJjY21jl1zzTWXPW5WVpZmzpxZT6sCYKKUyW/4ewpAQCqcO8LfU5AU4EE0ZMgQ6+ekpCQlJyfr+uuvV35+vvr06VNvj5uZman09HRr2+PxKCEhod4eDwAA+FfAv2X2Tdddd51at26tgwcPSpIcDofKysp8xly4cEEnT560rjtyOBwqLS31GXNp+7uuTbLZbLLb7T43AADQeF1VQfTPf/5TJ06cUFxcnCTJ6XSqvLxchYWF1pgNGzaopqZG3bt3t8Zs2rRJ58+ft8bk5uaqY8eOV3y7DAAAmMevQXT69GkVFRWpqKhIklRcXKyioiKVlJTo9OnTmjx5srZt26bDhw8rLy9P9957rzp06CCXyyVJ6ty5s/r376+xY8dqx44d2rJliyZMmKAhQ4YoPj5ekjR06FCFhoZqzJgx2rNnj95++20tWLDA5y0xAABgNr8G0eeff65bbrlFt9xyiyQpPT1dt9xyi6ZNm6aQkBDt3LlTv/rVr3TjjTdqzJgxSklJ0aeffiqbzWadY/ny5erUqZP69OmjgQMH6o477vD5jqHIyEitW7dOxcXFSklJ0dNPP61p06bxkXsAAGDx60XVvXv3ltfr/c7ja9eu/cFzREVFacWKFd87Jjk5WZ9++ulPnh8AADDDVXUNEQAAQH0giAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMbzaxBt2rRJgwYNUnx8vIKCgvT+++/7HPd6vZo2bZri4uIUFhamvn376sCBAz5jTp48qWHDhslut6tly5YaM2aMTp8+7TNm586d6tmzp5o1a6aEhATNmTOnvpcGAACuIn4NosrKSnXt2lWLFy++4vE5c+Zo4cKFWrp0qbZv367mzZvL5XLp3Llz1phhw4Zpz549ys3N1erVq7Vp0yY99thj1nGPx6N+/fqpXbt2Kiws1Ny5czVjxgy9/PLL9b4+AABwdWjizwcfMGCABgwYcMVjXq9XL7zwgqZOnap7771XkvTGG28oNjZW77//voYMGaIvv/xSa9as0WeffabbbrtNkrRo0SINHDhQzz33nOLj47V8+XJVV1frtddeU2hoqG666SYVFRXp+eef9wknAABgroC9hqi4uFhut1t9+/a19kVGRqp79+4qKCiQJBUUFKhly5ZWDElS3759FRwcrO3bt1tjevXqpdDQUGuMy+XS/v379fXXX1/xsauqquTxeHxuAACg8QrYIHK73ZKk2NhYn/2xsbHWMbfbrZiYGJ/jTZo0UVRUlM+YK53jm4/xbVlZWYqMjLRuCQkJP39BAAAgYAVsEPlTZmamKioqrNuRI0f8PSUAAFCPAjaIHA6HJKm0tNRnf2lpqXXM4XCorKzM5/iFCxd08uRJnzFXOsc3H+PbbDab7Ha7zw0AADReARtE7du3l8PhUF5enrXP4/Fo+/btcjqdkiSn06ny8nIVFhZaYzZs2KCamhp1797dGrNp0yadP3/eGpObm6uOHTvqmmuuaaDVAACAQObXIDp9+rSKiopUVFQk6V8XUhcVFamkpERBQUGaOHGifv/73+vDDz/Url27NGLECMXHx+u+++6TJHXu3Fn9+/fX2LFjtWPHDm3ZskUTJkzQkCFDFB8fL0kaOnSoQkNDNWbMGO3Zs0dvv/22FixYoPT0dD+tGgAABBq/fuz+888/15133mltX4qUkSNHKjs7W1OmTFFlZaUee+wxlZeX64477tCaNWvUrFkz6z7Lly/XhAkT1KdPHwUHB2vw4MFauHChdTwyMlLr1q1TWlqaUlJS1Lp1a02bNo2P3AMAAEuQ1+v1+nsSgc7j8SgyMlIVFRX1ej1RyuQ36u3cwNWscO4If0/hZ+P5DVxZfT6/f8rv74C9hggAAKChEEQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjBXQQzZgxQ0FBQT63Tp06WcfPnTuntLQ0tWrVShERERo8eLBKS0t9zlFSUqLU1FSFh4crJiZGkydP1oULFxp6KQAAIIA18fcEfshNN92k9evXW9tNmvzflCdNmqScnBy98847ioyM1IQJE/TAAw9oy5YtkqSLFy8qNTVVDodDW7du1bFjxzRixAg1bdpUzzzzTIOvBQAABKaAD6ImTZrI4XBctr+iokL//d//rRUrVuiuu+6SJL3++uvq3Lmztm3bph49emjdunXau3ev1q9fr9jYWN18882aPXu2MjIyNGPGDIWGhjb0cgAAQAAK6LfMJOnAgQOKj4/Xddddp2HDhqmkpESSVFhYqPPnz6tv377W2E6dOqlt27YqKCiQJBUUFCgpKUmxsbHWGJfLJY/Hoz179nznY1ZVVcnj8fjcAABA4xXQQdS9e3dlZ2drzZo1WrJkiYqLi9WzZ0+dOnVKbrdboaGhatmypc99YmNj5Xa7JUlut9snhi4dv3Tsu2RlZSkyMtK6JSQk1O3CAABAQAnot8wGDBhg/ZycnKzu3burXbt2WrlypcLCwurtcTMzM5Wenm5tezweoggAgEYsoF8h+raWLVvqxhtv1MGDB+VwOFRdXa3y8nKfMaWlpdY1Rw6H47JPnV3avtJ1SZfYbDbZ7XafGwAAaLyuqiA6ffq0Dh06pLi4OKWkpKhp06bKy8uzju/fv18lJSVyOp2SJKfTqV27dqmsrMwak5ubK7vdrsTExAafPwAACEwB/ZbZf/7nf2rQoEFq166djh49qunTpyskJESPPPKIIiMjNWbMGKWnpysqKkp2u11PPvmknE6nevToIUnq16+fEhMTNXz4cM2ZM0dut1tTp05VWlqabDabn1cHAAACRUAH0T//+U898sgjOnHihKKjo3XHHXdo27Ztio6OliTNnz9fwcHBGjx4sKqqquRyufTSSy9Z9w8JCdHq1as1fvx4OZ1ONW/eXCNHjtSsWbP8tSQAABCAAjqI3nrrre893qxZMy1evFiLFy/+zjHt2rXTxx9/XNdTAwAAjchVdQ0RAABAfSCIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxjMqiBYvXqxrr71WzZo1U/fu3bVjxw5/TwkAAAQAY4Lo7bffVnp6uqZPn66//e1v6tq1q1wul8rKyvw9NQAA4GfGBNHzzz+vsWPH6je/+Y0SExO1dOlShYeH67XXXvP31AAAgJ818fcEGkJ1dbUKCwuVmZlp7QsODlbfvn1VUFBw2fiqqipVVVVZ2xUVFZIkj8dTr/O8WHW2Xs8PXK3q+7nXEHh+A1dWn8/vS+f2er0/ONaIIPrf//1fXbx4UbGxsT77Y2NjtW/fvsvGZ2VlaebMmZftT0hIqLc5AvhukYvG+XsKAOpJQzy/T506pcjIyO8dY0QQ/VSZmZlKT0+3tmtqanTy5Em1atVKQUFBfpwZGoLH41FCQoKOHDkiu93u7+kAqEM8v83i9Xp16tQpxcfH/+BYI4KodevWCgkJUWlpqc/+0tJSORyOy8bbbDbZbDaffS1btqzPKSIA2e12/oMJNFI8v83xQ68MXWLERdWhoaFKSUlRXl6eta+mpkZ5eXlyOp1+nBkAAAgERrxCJEnp6ekaOXKkbrvtNv3bv/2bXnjhBVVWVuo3v/mNv6cGAAD8zJggevjhh3X8+HFNmzZNbrdbN998s9asWXPZhdaAzWbT9OnTL3vbFMDVj+c3vkuQ98d8Fg0AAKARM+IaIgAAgO9DEAEAAOMRRAAAwHgEERq13r17a+LEif6eBgAgwBFEAADAeAQRAAAwHkGERq+mpkZTpkxRVFSUHA6HZsyYYR17/vnnlZSUpObNmyshIUFPPPGETp8+bR3Pzs5Wy5YttXr1anXs2FHh4eF68MEHdebMGS1btkzXXnutrrnmGj311FO6ePGiH1YHmOXdd99VUlKSwsLC1KpVK/Xt21eVlZUaNWqU7rvvPs2cOVPR0dGy2+0aN26cqqurrfuuWbNGd9xxh1q2bKlWrVrpnnvu0aFDh6zjhw8fVlBQkFauXKmePXsqLCxM3bp101dffaXPPvtMt912myIiIjRgwAAdP37cH8tHPSKI0OgtW7ZMzZs31/bt2zVnzhzNmjVLubm5kqTg4GAtXLhQe/bs0bJly7RhwwZNmTLF5/5nzpzRwoUL9dZbb2nNmjXKz8/X/fffr48//lgff/yx/vznP+tPf/qT3n33XX8sDzDGsWPH9Mgjj2j06NH68ssvlZ+frwceeECXvk4vLy/P2v/mm2/qvffe08yZM637V1ZWKj09XZ9//rny8vIUHBys+++/XzU1NT6PM336dE2dOlV/+9vf1KRJEw0dOlRTpkzRggUL9Omnn+rgwYOaNm1ag64dDcALNGK//OUvvXfccYfPvm7dunkzMjKuOP6dd97xtmrVytp+/fXXvZK8Bw8etPY9/vjj3vDwcO+pU6esfS6Xy/v444/X8ewBfFNhYaFXkvfw4cOXHRs5cqQ3KirKW1lZae1bsmSJNyIiwnvx4sUrnu/48eNeSd5du3Z5vV6vt7i42CvJ++qrr1pj3nzzTa8kb15enrUvKyvL27Fjx7paFgIErxCh0UtOTvbZjouLU1lZmSRp/fr16tOnj37xi1+oRYsWGj58uE6cOKEzZ85Y48PDw3X99ddb27Gxsbr22msVERHhs+/SOQHUj65du6pPnz5KSkrSQw89pFdeeUVff/21z/Hw8HBr2+l06vTp0zpy5Igk6cCBA3rkkUd03XXXyW6369prr5UklZSU+DzON/+bcenPOyUlJfns4/ne+BBEaPSaNm3qsx0UFKSamhodPnxY99xzj5KTk/WXv/xFhYWFWrx4sST5XHdwpft/1zkB1J+QkBDl5ubqk08+UWJiohYtWqSOHTuquLj4R91/0KBBOnnypF555RVt375d27dvl+T7fJd8n/NBQUFX3MfzvfEx5o+7At9WWFiompoazZs3T8HB//p/g5UrV/p5VgC+T1BQkG6//XbdfvvtmjZtmtq1a6dVq1ZJkr744gudPXtWYWFhkqRt27YpIiJCCQkJOnHihPbv369XXnlFPXv2lCRt3rzZb+tA4CGIYKwOHTro/PnzWrRokQYNGqQtW7Zo6dKl/p4WgO+wfft25eXlqV+/foqJidH27dt1/Phxde7cWTt37lR1dbXGjBmjqVOn6vDhw5o+fbomTJig4OBgXXPNNWrVqpVefvllxcXFqaSkRP/1X//l7yUhgPCWGYzVtWtXPf/88/rjH/+oLl26aPny5crKyvL3tAB8B7vdrk2bNmngwIG68cYbNXXqVM2bN08DBgyQJPXp00c33HCDevXqpYcffli/+tWvrK/ZCA4O1ltvvaXCwkJ16dJFkyZN0ty5c/24GgSaIK/3/39eEQCAq9SoUaNUXl6u999/399TwVWKV4gAAIDxCCIAAGA83jIDAADG4xUiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gghAo9C7d29NnDjxR43Nz89XUFCQysvLf9ZjXnvttXrhhRd+1jkABAaCCAAAGI8gAgAAxiOIADQ6f/7zn3XbbbepRYsWcjgcGjp0qMrKyi4bt2XLFiUnJ6tZs2bq0aOHdu/e7XN88+bN6tmzp8LCwpSQkKCnnnpKlZWVDbUMAA2IIALQ6Jw/f16zZ8/WF198offff1+HDx/WqFGjLhs3efJkzZs3T5999pmio6M1aNAgnT9/XpJ06NAh9e/fX4MHD9bOnTv19ttva/PmzZowYUIDrwZAQ2ji7wkAQF0bPXq09fN1112nhQsXqlu3bjp9+rQiIiKsY9OnT9fdd98tSVq2bJnatGmjVatW6de//rWysrI0bNgw60LtG264QQsXLtQvf/lLLVmyRM2aNWvQNQGoX7xCBKDRKSws1KBBg9S2bVu1aNFCv/zlLyVJJSUlPuOcTqf1c1RUlDp27Kgvv/xSkvTFF18oOztbERER1s3lcqmmpkbFxcUNtxgADYJXiAA0KpWVlXK5XHK5XFq+fLmio6NVUlIil8ul6urqH32e06dP6/HHH9dTTz112bG2bdvW5ZQBBACCCECjsm/fPp04cULPPvusEhISJEmff/75Fcdu27bNipuvv/5aX331lTp37ixJuvXWW7V371516NChYSYOwK94ywxAo9K2bVuFhoZq0aJF+sc//qEPP/xQs2fPvuLYWbNmKS8vT7t379aoUaPUunVr3XfffZKkjIwMbd26VRMmTFBRUZEOHDigDz74gIuqgUaKIALQqERHRys7O1vvvPOOEhMT9eyzz+q555674thnn31W//Ef/6GUlBS53W599NFHCg0NlSQlJydr48aN+uqrr9SzZ0/dcsstmjZtmuLj4xtyOQAaSJDX6/X6exIAAAD+xCtEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjPf/AKpWu6SYWEvyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(emails, x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Định nghĩa một hàm `preprocess_text()` để tiền xử lý dữ liệu văn bản bằng cách loại bỏ dấu câu, phân tách từ, loại bỏ từ dừng và áp dụng stemming.\n",
    "\n",
    "- Sau đó, hàm này sẽ được áp dụng cho cột 'text' của một DataFrame có tên là `emails`, tạo ra một cột mới có tên `cleaned_text` chứa văn bản đã được xử lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject enron methanol meter 988291 follow not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject hpl nom januari 9 2001 see attach file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject neon retreat ho ho ho around wonder ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject photoshop window offic cheap main tren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject indian spring deal book teco pvr reven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num                                       cleaned_text  \n",
       "0          0  subject enron methanol meter 988291 follow not...  \n",
       "1          0  subject hpl nom januari 9 2001 see attach file...  \n",
       "2          0  subject neon retreat ho ho ho around wonder ti...  \n",
       "3          1  subject photoshop window offic cheap main tren...  \n",
       "4          0  subject indian spring deal book teco pvr reven...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Tokenize words\n",
    "    words = text.split()\n",
    "    # Remove stopwords and stemming\n",
    "    stop_words = stopwords.words('english')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "emails['cleaned_text'] = emails['text'].apply(preprocess_text)\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diễn giải thêm về hàm này:\n",
    "\n",
    "1. Function Definition (preprocess_text) : Hàm này nhận một tham số duy nhất, `text`, được mong đợi là một chuỗi (nội dung của một email trong trường hợp này).\n",
    "\n",
    "2. Removing Punctuation :\n",
    "- Đoạn code `text = ''.join([char for char in text if char not in string.punctuation])` lặp qua từng ký tự trong văn bản đầu vào và giữ lại chỉ những ký tự không phải là dấu câu.\n",
    "- Việc này được thực hiện bằng cách sử dụng một comprehension danh sách và mô-đun `string.punctuation`, chứa tất cả các ký hiệu dấu câu thông dụng.\n",
    "\n",
    "3. Tokenizing Words : Sau khi loại bỏ dấu câu, `text.split()` được sử dụng để phân tách văn bản thành các từ riêng lẻ (tokens) dựa trên khoảng trắng. Điều này tạo ra một danh sách các từ.\n",
    "\n",
    "4. Removing Stopwords and Stemming :\n",
    "- Stopwords là những từ thông dụng (như \"và\", \"cái\", \"là\", v.v.) thường được loại bỏ khỏi dữ liệu văn bản vì chúng không đóng góp nhiều vào ý nghĩa của văn bản. Hàm `stopwords.words('english')` từ thư viện nltk (Natural Language Toolkit) cung cấp một danh sách các stopwords trong tiếng Anh.\n",
    "\n",
    "- Stemming là quá trình rút gọn một từ về dạng cơ sở hoặc gốc của nó. Ví dụ, \"running\" trở thành \"run.\" `SnowballStemmer` là một loại stemmer được cung cấp bởi thư viện nltk, được sử dụng ở đây để thực hiện quá trình stemming.\n",
    "\n",
    "- Đoạn code `words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]` thực hiện cả việc loại bỏ stopwords và stemming. Nó lặp qua danh sách các từ, chuyển mỗi từ thành chữ thường, kiểm tra xem nó không phải là stopwords, và sau đó thực hiện stemming bằng cách sử dụng `SnowballStemmer`.\n",
    "\n",
    "5. Joining Words Back into a String : Các từ đã được tiền xử lý sau đó được nối lại thành một chuỗi duy nhất bằng cách sử dụng `' '.join(words)` để tạo ra phiên bản văn bản đã được làm sạch.\n",
    "\n",
    "6. Applying the Function to the Dataset :\n",
    "`emails['cleaned_text'] = emails['text'].apply(preprocess_text)` áp dụng hàm `preprocess_text` cho từng phần tử trong cột văn bản của DataFrame `emails`. Kết quả được lưu trữ trong một cột mới có tên là `cleaned_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mã hóa vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thực hiện việc trích xuất đặc trưng sử dụng Vectorizer TF-IDF để chuyển đổi dữ liệu văn bản đã được làm sạch thành dạng số có thể được sử dụng bởi các thuật toán học máy.\n",
    "\n",
    "Viết tắt của thuật ngữ tiếng Anh term frequency – inverse document frequency,tf-idf là trọng số của một từ trong văn bản thu được qua thống kê thể hiện mức độ quan trọng của từ này trong một văn bản, mà bản thân văn bản đang xét nằm trong một tập hợp các văn bản.\n",
    "\n",
    "Thuật toán này thường được sử dụng vì: trong ngôn ngữ luôn có những từ xảy ra thường xuyên với các từ khác.\n",
    "\n",
    "1. Tf- term frequency : dùng để ước lượng tần xuất xuất hiện của từ trong văn bản. Tuy nhiên với mỗi văn bản thì có độ dài khác nhau, vì thế số lần xuất hiện của từ có thể nhiều hơn . Vì vậy số lần xuất hiện của từ sẽ được chia độ dài của văn bản (tổng số từ trong văn bản đó). \n",
    "\n",
    "`TF(t, d) = ( số lần từ t xuất hiện trong văn bản d) / (tổng số từ trong văn bản d)`\n",
    "\n",
    "2. IDF- Inverse Document Frequency: dùng để ước lượng mức độ quan trọng của từ đó như thế nào . Khi tính tần số xuất hiện tf thì các từ đều được coi là quan trọng như nhau. Tuy nhiên có một số từ thường được được sử dụng nhiều nhưng không quan trọng để thể hiện ý nghĩa của đoạn văn , ví dụ :\n",
    "Từ nối: và, nhưng, tuy nhiên, vì thế, vì vậy, …\n",
    "* Giới từ: ở, trong, trên, …\n",
    "* Từ chỉ định: ấy, đó, nhỉ, …\n",
    "* Vì vậy ta cần giảm đi mức độ quan trọng của những từ đó bằng cách sử dụng IDF :\n",
    "\n",
    "`IDF(t, D) = log_e( Tổng số văn bản trong tập mẫu D/ Số văn bản có chứa từ t )`\n",
    "\n",
    "`TfidfVectorizer` là một kỹ thuật trích xuất đặc trưng từ thư viện scikit-learn, chuyển đổi dữ liệu văn bản thành các đặc trưng số dựa trên điểm số TF-IDF. TF-IDF đo lường tầm quan trọng của một từ trong một tài liệu so với sự xuất hiện của nó trong toàn bộ tập hợp tài liệu. Kỹ thuật này giúp giảm trọng số của những từ thường gặp (như \"cái,\" \"và,\" v.v.) mà ít thông tin hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = tfidf_vectorizer.fit_transform(emails['cleaned_text']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tfidf_vectorizer = TfidfVectorizer(max_features=3000)` khởi tạo vectorizer với tối đa 5.000 đặc trưng. Điều này có nghĩa là nó sẽ chỉ xem xét 5.000 từ hàng đầu (dựa trên điểm số TF-IDF của chúng) làm đặc trưng, giúp giảm chiều dữ liệu và chi phí tính toán.\n",
    "\n",
    "- `X = tfidf_vectorizer.fit_transform(emails['cleaned_text']).toarray()` áp dụng biến đổi TF-IDF cho cột `cleaned_text` của DataFrame `emails`. Phương thức `fit_transform` học từ vựng từ dữ liệu văn bản và chuyển đổi nó thành một ma trận tài liệu-thuật ngữ được trọng số bằng TF-IDF. Kết quả sau đó được chuyển đổi thành một mảng NumPy bằng cách sử dụng `.toarray()`, tạo ra một ma trận `X` trong đó mỗi hàng đại diện cho một email, và mỗi cột đại diện cho một đặc trưng TF-IDF tương ứng với một từ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chia tập dữ liệu huấn luyện, kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = emails['label_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Target Variable (y): `y = emails['label_num']` gán biến mục tiêu `y` là cột `label_num` từ DataFrame `emails`. Cột này có thể chứa các nhãn số đại diện cho phân loại của các email, trong đó mỗi email được gán nhãn là \"spam\" (ví dụ: 1) hoặc \"ham\" (không phải spam, ví dụ: 0).\n",
    "\n",
    "2. Splitting Data into Training and Testing Sets:\n",
    "- `train_test_split` là một hàm từ scikit-learn dùng để chia tập dữ liệu thành các tập huấn luyện và kiểm tra. Mô hình được huấn luyện trên tập huấn luyện và được đánh giá trên tập kiểm tra để đánh giá khả năng tổng quát của nó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng cây quyết định (Decision Tree) để huấn luyện dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cơ sở lý thuyết về cây quyết định"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree (cây quyết định) là một mô hình học máy đưa ra kết quả dựa trên các câu hỏi. Nó có dạng như hình bên dưới\n",
    "<img src=\"decision_tree_sample.png\" width = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong decision tree, các ô màu xám, lục, đỏ trên hình trên được gọi là các node. Các node thể hiện đầu ra (màu lục và đỏ) được gọi là `node lá` (`leaf node` hoặc `terminal node`). Các node thể hiện câu hỏi là các `non-leaf node`. \n",
    "\n",
    "`Non-leaf node` trên cùng (câu hỏi đầu tiên) được gọi là node gốc (`root node`). Các `non-leaf node` thường có hai hoặc nhiều node con (`child node`). Các child node này có thể là một leaf node hoặc một non-leaf node khác. Các child node có cùng bố mẹ được gọi là sibling node. Nếu tất cả các non-leaf node chỉ có hai child node, ta nói rằng đó là một binary decision tree (cây quyết định nhị phân). \n",
    "\n",
    "Các câu hỏi trong binary decision tree đều có thể đưa được về dạng câu hỏi đúng hay sai. Các decision tree mà một leaf node có nhiều child node cũng có thể được đưa về dạng một binary decision tree. Điều này có thể đạt được vì hầu hết các câu hỏi đều có thể được đưa về dạng câu hỏi đúng sai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thuật toán ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ý tưởng chính"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ID3, chúng ta cần xác định thứ tự của thuộc tính cần được xem xét tại mỗi bước. Với các bài toán có nhiều thuộc tính và mỗi thuộc tính có nhiều giá trị khác nhau, việc tìm được nghiệm tối ưu thường là không khả thi. Thay vào đó, một phương pháp đơn giản thường được sử dụng là tại mỗi bước, một thuộc tính tốt nhất sẽ được chọn ra dựa trên một tiêu chuẩn nào đó (chúng ta sẽ bàn sớm). Với mỗi thuộc tính được chọn, ta chia dữ liệu vào các child node tương ứng với các giá trị của thuộc tính đó rồi tiếp tục áp dụng phương pháp này cho mỗi child node. Việc chọn ra thuộc tính tốt nhất ở mỗi bước như thế này được gọi là cách chọn greedy (tham lam). Cách chọn này có thể không phải là tối ưu, nhưng trực giác cho chúng ta thấy rằng cách làm này sẽ gần với cách làm tối ưu. Ngoài ra, cách làm này khiến cho bài toán cần giải quyết trở nên đơn giản hơn.\n",
    "\n",
    "Sau mỗi câu hỏi, dữ liệu được phân chia vào từng child node tương ứng với các câu trả lời cho câu hỏi đó. Câu hỏi ở đây chính là một thuộc tính, câu trả lời chính là giá trị của thuộc tính đó. Để đánh giá chất lượng của một cách phân chia, chúng ta cần đi tìm một phép đo.\n",
    "\n",
    "Trước hết, thế nào là một phép phân chia tốt? Bằng trực giác, một phép phân chia là tốt nhất nếu dữ liệu trong mỗi child node hoàn toàn thuộc vào một class–khi đó child node này có thể được coi là một leaf node, tức ta không cần phân chia thêm nữa. Nếu dữ liệu trong các child node vẫn lẫn vào nhau theo tỉ lệ lớn, ta coi rằng phép phân chia đó chưa thực sự tốt. Từ nhận xét này, ta cần có một hàm số đo độ tinh khiết (purity), hoặc độ vẩn đục (impurity) của một phép phân chia. Hàm số này sẽ cho giá trị thấp nhất nếu dữ liệu trong mỗi child node nằm trong cùng một class (tinh khiết nhất), và cho giá trị cao nếu mỗi child node có chứa dữ liệu thuộc nhiều class khác nhau.\n",
    "\n",
    "Một hàm số có các đặc điểm này và được dùng nhiều trong lý thuyết thông tin là hàm entropy. Hàm số entropy được diễn tả trong hình dưới\n",
    "\n",
    "<img src=\"entropy_func.png\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thuật toán"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ID3, tổng có trọng số của entropy tại các leaf-node sau khi xây dựng decision tree được coi là hàm mất mát của decision tree đó. Các trọng số ở đây tỉ lệ với số điểm dữ liệu được phân vào mỗi node. Công việc của ID3 là tìm các cách phân chia hợp lý (thứ tự chọn thuộc tính hợp lý) sao cho hàm mất mát cuối cùng đạt giá trị càng nhỏ càng tốt. Như đã đề cập, việc này đạt được bằng cách chọn ra thuộc tính sao cho nếu dùng thuộc tính đó để phân chia, entropy tại mỗi bước giảm đi một lượng lớn nhất. Bài toán xây dựng một decision tree bằng ID3 có thể chia thành các bài toán nhỏ, trong mỗi bài toán, ta chỉ cần chọn ra thuộc tính giúp cho việc phân chia đạt kết quả tốt nhất. Mỗi bài toán nhỏ này tương ứng với việc phân chia dữ liệu trong một non-leaf node. Chúng ta sẽ xây dựng phương pháp tính toán dựa trên mỗi node này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước hết, **tại mỗi node**:\n",
    "\n",
    "Xét một bài toán với $C$ class khác nhau. Giả sử ta đang làm việc với một non-leaf node với các điểm dữ liệu tạo thành một tập $\\mathcal{S}$ với số phần tử là $N$. Trong các điểm này có $N_c$ điểm thuộc class $c$, với mọi $c\\in \\overline{1,C}$. Xác suất mỗi điểm rơi vào class $c$ được xấp xỉ bởi $\\dfrac{N_c}{N}$. Như vậy entropy tại node này là\n",
    "\\begin{align*}\n",
    "H(\\mathcal{S}) = -\\sum_{c=1}^C \\frac{N_C}{N}\\log{\\frac{N_c}{N}}\\tag{1}\n",
    "\\end{align*}\n",
    "Tiếp theo, giả sử thuộc tính được chọn để chia dữ liệu là $x$. Dựa trên $x$, các điểm dữ liệu trong $\\mathcal{F}$ được phân ra thành $K$ childnode là $\\mathcal{S}_1,...,\\mathcal{S_K}$ với số điểm trong mỗi childnode là $m_1,...,m_K$. Ta tính đại lượng để đo mức độ vẩn đục của một phép chia dựa vào $x$ trên $\\mathcal{S}$ là tổng có trọng số các entropy tại $\\mathcal{S}_k$ (được tính như (1)).\n",
    "\\begin{align*}\n",
    "H(x, \\mathcal{S}) = \\sum_{k=1}^K\\frac{m_k}{N}H(S_k)\n",
    "\\end{align*}\n",
    "Và ta định nghĩa `gain` của node $\\mathcal{S}$ dựa trên việc phân chia theo thuộc tính $x$ là\n",
    "\\begin{align*}\n",
    "G(x, \\mathcal{S}) = H(\\mathcal{S}) - H(x, \\mathcal{S})\n",
    "\\end{align*}\n",
    "Trong thuật toán ID3, tại mỗi node khi phân chia, thuộc tính $x$ được chọn để phân chia là thuộc tính mà có $G(x,\\mathcal{S})$ đạt giá trị lớn nhất.\n",
    "\n",
    "**Điểm dừng** của thuật toán: \n",
    "\n",
    "Trong các thuật toán decision tree nói chung và ID3 nói riêng, nếu ta tiếp tục phân chia các node chưa tinh khiết, ta sẽ thu được một tree mà mọi điểm trong tập huấn luyện đều được dự đoán đúng (giả sử rằng không có hai input giống nhau nào cho output khác nhau). Khi đó, tree có thể sẽ rất phức tạp (nhiều node) với nhiều leaf node chỉ có một vài điểm dữ liệu. Như vậy, nhiều khả năng overfitting sẽ xảy ra.\n",
    "\n",
    "Để tránh overfitting, một trong số các phương pháp sau có thể được sử dụng. Tại một node, nếu một trong số các điều kiện sau đây xảy ra, ta không tiếp tục phân chia node đó và coi nó là một leaf node:\n",
    "\n",
    "* nếu node đó có entropy bằng 0, tức mọi điểm trong node đều thuộc một class.\n",
    "\n",
    "* nếu node đó có số phần tử nhỏ hơn một ngưỡng nào đó. Trong trường hợp này, ta chấp nhận có một số điểm bị phân lớp sai để tránh overfitting. Class cho leaf node này có thể được xác định dựa trên class chiếm đa số trong node.\n",
    "\n",
    "* nếu khoảng cách từ node đó đến root node đạt tới một giá trị nào đó. Việc hạn chế chiều sâu của tree này làm giảm độ phức tạp của tree và phần nào giúp tránh overfitting.\n",
    "\n",
    "* nếu tổng số leaf node vượt quá một ngưỡng nào đó.\n",
    "\n",
    "* nếu việc phân chia node đó không làm giảm entropy quá nhiều (information gain nhỏ hơn một ngưỡng nào đó)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chương trình thực thi Decision Tree bằng thuật toán ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trước hết, ta tạo mỗi cấu trúc TreeNode với các thông tin cần thiết, bao gồm\n",
    "* Những điểm dữ liệu thuộc node này\n",
    "* Độ sâu của Node\n",
    "* Thuộc tính được chọn để chia nhỏ ở nút này (nếu không phải nút lá)\n",
    "* Danh sách các nút con sau khi phân chia\n",
    "* Các giá trị thuộc tính chia nhỏ (hay các ngưỡng chia) trong các nút con\n",
    "* Nhãn (nếu là nút lá)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # Đảm bảo tương thích với phiên bản Python 2 và 3\n",
    "\n",
    "class TreeNode(object):\n",
    "    def __init__(self, ids=None, children=[], entropy=0, depth=0):\n",
    "        self.ids = ids  # Danh sách các chỉ số dữ liệu trong nút này\n",
    "        self.entropy = entropy  # Độ đo entropy của nút, sẽ được tính sau\n",
    "        self.depth = depth  # Độ sâu của nút so với nút gốc\n",
    "        self.split_attribute = None  # Thuộc tính được chọn để chia nhỏ, nếu không phải là nút lá\n",
    "        self.children = children  # Danh sách các nút con của nút hiện tại\n",
    "        self.order = None  # Thứ tự các giá trị của thuộc tính chia nhỏ trong các nút con\n",
    "        self.label = None  # Nhãn của nút nếu nó là nút lá\n",
    "        \n",
    "    def set_properties(self, split_attribute, order):\n",
    "        self.split_attribute = split_attribute  \n",
    "        self.order = order  \n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo là hàm entropy để tính entropy của một phân phối theo công thức ở phần trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(freq):\n",
    "    freq_0 = freq[np.array(freq).nonzero()[0]]  # Loại bỏ tần số bằng 0\n",
    "    prob_0 = freq_0 / float(freq_0.sum())  # Tính xác suất của mỗi tần số\n",
    "    return -np.sum(prob_0 * np.log(prob_0))  # Tính entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng là lớp DecisionTreeID3 để minh họa cách xây dựng một Decision Tree bằng thuật toán ID3, nó sẽ gồm các siêu tham số sau để tránh overfitting (tương ứng những gì đã trình bày ở phần trước):\n",
    "* `max_depth`: giới hạn độ sâu của cây.\n",
    "* `min_samples_split`: số mẫu tối thiểu cần có ở mỗi node con khi áp dụng chia thuộc tính ở node cha.\n",
    "* `min_gain`: giá trị gain tối thiểu để áp dụng một cách phân chia ở một node.\n",
    "* `max_bin`: số lượng ngưỡng tối đa được tạo ra để chia tách một thuộc tính numeric (sẽ được trình bày kĩ hơn ở dưới).\n",
    "Lớp này gồm các phương thức quan trọng:\n",
    "* `fit(data, target)`: Xây dựng cây dựa trên dữ liệu huấn luyện là `data` và nhãn tương ứng là `target`. Nó sẽ lấy node gốc (root) là node chứa toàn bộ dữ liệu và bắt đầu phân chia dựa vào hàm `split`.\n",
    "* `split(node)`: Chia một Node bằng thuật toán ID3. Nó sẽ duyệt qua từng thuộc tính (đặc trưng) của dữ liệu và tính toán gain cho mỗi thuộc tính theo cách đã được trình bày ở phần trước. Cuối cùng, nó sẽ chọn thuộc tính mà sau khi sử dụng để chia thì có gain cao nhất và trả về danh sách các nút con được chia.\n",
    "\n",
    "Có một chú ý thêm rằng, trong khi chia:\n",
    "* Nếu thuộc tính là categorical thì giá trị thuộc tính tại các node con chính là các giá trị duy nhất có thể có của thuộc tính đó trong bộ dữ liệu.\n",
    "* Nếu thuộc tính là numeric thì sẽ dùng các ngưỡng để phân chia các dữ liệu vào các nút con (ví dụ `<7`, `>=7 & <8`, etc.). Ngưỡng được chọn chính là trung bình cộng 2 giá trị liên tiếp trong tập giá trị đã được sắp xếp của thuộc tính này. Tuy nhiên, nếu số lượng ngưỡng được tạo ra vượt qua `max_bin` thì các ngưỡng sẽ được chọn ngẫu nhiên để đảm bảo có nhiều nhất `max_bin` ngưỡng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeID3(object):\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, min_gain=1e-4, max_bin=10):\n",
    "        self.root = None  # Nút gốc của cây quyết định\n",
    "        self.max_depth = max_depth  # Độ sâu tối đa của cây\n",
    "        self.min_samples_split = min_samples_split  # Số mẫu tối thiểu để chia nhỏ một nút\n",
    "        self.Ntrain = 0  # Số lượng mẫu trong tập huấn luyện\n",
    "        self.min_gain = min_gain  # Mức gain tối thiểu để chia nhỏ tiếp tục\n",
    "        self.max_bin = max_bin # Số lượng ngưỡng tối đa để phân chia dữ liệu liên tục\n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        self.Ntrain = data.count()[0]  # Đếm số lượng mẫu trong tập dữ liệu\n",
    "        self.data = data  \n",
    "        self.attributes = list(data)  # Lấy danh sách các thuộc tính\n",
    "        self.target = target  \n",
    "        self.labels = target.unique()  # Lấy các nhãn mục tiêu duy nhất\n",
    "        \n",
    "        ids = range(self.Ntrain)  # Tạo danh sách các chỉ số mẫu dữ liệu\n",
    "        self.root = TreeNode(ids=ids, entropy=self._entropy(ids), depth=0)  # Tạo nút gốc với toàn bộ dữ liệu\n",
    "        queue = [self.root]  \n",
    "        while queue:\n",
    "            node = queue.pop()  \n",
    "            # Nếu chưa đạt tới độ sâu giới hạn hoặc entropy vẫn lớn hơn mức thông tin nhỏ nhất để chia\n",
    "            if node.depth < self.max_depth or node.entropy < self.min_gain:\n",
    "                node.children = self._split(node)  \n",
    "                if not node.children:  # Nếu không có nút con, đặt nhãn cho nút\n",
    "                    self._set_label(node)\n",
    "                queue += node.children  # Thêm các nút con vào hàng đợi\n",
    "            else:\n",
    "                self._set_label(node)  # Nếu không thể chia nhỏ thêm, đặt nhãn cho nút\n",
    "                \n",
    "    def _entropy(self, ids):\n",
    "        if len(ids) == 0:\n",
    "            return 0\n",
    "        freq = np.array(self.target.iloc[ids].value_counts())  # Đếm tần số của các nhãn mục tiêu\n",
    "        return entropy(freq)  # Tính entropy dựa trên tần số\n",
    "    \n",
    "    def _set_label(self, node):\n",
    "        target_ids = node.ids\n",
    "        node.set_label(self.target.iloc[target_ids].mode()[0])  # Đặt nhãn cho nút lá bằng giá trị mode của các nhãn mục tiêu\n",
    "    \n",
    "    def _split(self, node):\n",
    "        ids = node.ids  # Lấy danh sách các chỉ số dữ liệu trong nút hiện tại\n",
    "        best_gain = 0  \n",
    "        best_splits = []  # Khởi tạo danh sách các cách chia nhỏ tốt nhất\n",
    "        best_attribute = None \n",
    "        order = None  \n",
    "        sub_data = self.data.iloc[ids, :]  # Lấy tập dữ liệu con tương ứng với các chỉ số dữ liệu của nút hiện tại\n",
    "\n",
    "        for i, att in enumerate(self.attributes):  # Duyệt qua từng đặc trưng \n",
    "            values = sub_data[att].unique()  \n",
    "            if len(values) == 1:  # Nếu đặc trưng chỉ có một giá trị duy nhất, bỏ qua đặc trưng này\n",
    "                continue\n",
    "            if np.issubdtype(values.dtype, np.number):  # Kiểm tra nếu thuộc tính là liên tục\n",
    "                # Lấy các ngưỡng chia \n",
    "                sorted_values = np.sort(values)\n",
    "                thresholds = [(sorted_values[j] + sorted_values[j + 1]) / 2 for j in range(len(sorted_values) - 1)]\n",
    "                # Giới hạn ngưỡng chia\n",
    "                if len(thresholds) > self.max_bin:\n",
    "                    thresholds = np.random.choice(thresholds, self.max_bin, replace=False)\n",
    "                    \n",
    "                for threshold in thresholds:  # Duyệt qua các ngưỡng để kiểm tra\n",
    "                    splits = [\n",
    "                        [self.data.index.get_loc(idx) for idx in sub_data.index[sub_data[att] <= threshold]],  # Dữ liệu có giá trị <= ngưỡng\n",
    "                        [self.data.index.get_loc(idx) for idx in sub_data.index[sub_data[att] > threshold]]  # Dữ liệu có giá trị > ngưỡng\n",
    "                    ]\n",
    "                    if min(map(len, splits)) < self.min_samples_split:  # Kiểm tra nếu một trong các cách chia nhỏ có số lượng mẫu nhỏ hơn min_samples_split\n",
    "                        continue\n",
    "                    HxS = 0  # Khởi tạo tổng entropy có trọng số\n",
    "                    for split in splits:  \n",
    "                        HxS += len(split) * self._entropy(split) / len(ids)  # Tính entropy có trọng số của cách chia nhỏ hiện tại\n",
    "                    gain = node.entropy - HxS  # Tính gain cho cách chia nhỏ hiện tại\n",
    "                    if gain < self.min_gain:  \n",
    "                        continue\n",
    "                    if gain > best_gain:  \n",
    "                        best_gain = gain\n",
    "                        best_splits = splits\n",
    "                        best_attribute = att\n",
    "                        order = [f\"<= {threshold}\", f\"> {threshold}\"]\n",
    "            else:  # Xử lý các thuộc tính categorical\n",
    "                splits = []  \n",
    "                for val in values:  \n",
    "                    sub_ids = sub_data.index[sub_data[att] == val].tolist()  # Lấy danh sách các chỉ số dữ liệu có giá trị hiện tại của đặc trưng\n",
    "                    splits.append([self.data.index.get_loc(idx) for idx in sub_ids])  # Thêm danh sách chỉ số vào danh sách các cách chia nhỏ\n",
    "                if min(map(len, splits)) < self.min_samples_split:  \n",
    "                    continue\n",
    "                HxS = 0  # Khởi tạo tổng entropy có trọng số\n",
    "                for split in splits:  # Duyệt qua từng cách chia nhỏ\n",
    "                    HxS += len(split) * self._entropy(split) / len(ids)  # Tính entropy có trọng số của cách chia nhỏ hiện tại\n",
    "                gain = node.entropy - HxS  # Tính gain cho cách chia nhỏ hiện tại\n",
    "                if gain < self.min_gain:  \n",
    "                    continue\n",
    "                if gain > best_gain:  \n",
    "                    best_gain = gain\n",
    "                    best_splits = splits\n",
    "                    best_attribute = att\n",
    "                    order = values\n",
    "                    \n",
    "        node.set_properties(best_attribute, order)  # Thiết lập thuộc tính chia nhỏ và thứ tự giá trị tốt nhất cho nút hiện tại\n",
    "        child_nodes = [\n",
    "            TreeNode(ids=split, entropy=self._entropy(split), depth=node.depth + 1)  \n",
    "            for split in best_splits\n",
    "        ]\n",
    "        return child_nodes  # Trả về danh sách các nút con\n",
    "\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        :param new_data: a new dataframe, each row is a datapoint\n",
    "        :return: predicted labels for each row\n",
    "        \"\"\"\n",
    "        npoints = new_data.count()[0]\n",
    "        labels = [None] * npoints\n",
    "        for n in range(npoints):\n",
    "            x = new_data.iloc[n, :]  # Lấy một điểm dữ liệu \n",
    "            node = self.root\n",
    "            while node.children:\n",
    "                if isinstance(node.order[0], str) and \"<=\" in node.order[0]:  # Kiểm tra nếu thuộc tính là liên tục\n",
    "                    threshold = float(node.order[0].split(\" \")[1])  # Tách ngưỡng từ chuỗi\n",
    "                    if x[node.split_attribute] <= threshold:\n",
    "                        node = node.children[0]  # Đi xuống nhánh trái nếu giá trị nhỏ hơn hoặc bằng ngưỡng\n",
    "                    else:\n",
    "                        node = node.children[1]  # Đi xuống nhánh phải nếu giá trị lớn hơn ngưỡng\n",
    "                else:\n",
    "                    node = node.children[node.order.index(x[node.split_attribute])]  # Xử lý thuộc tính categorical\n",
    "            labels[n] = node.label  \n",
    "            \n",
    "        return labels  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phân tích và đánh giá kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.019495</td>\n",
       "      <td>0.062764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.577269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.079235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.629499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.016227</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4136.0</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count      mean       std  min  25%  50%  75%       max\n",
       "0  4136.0  0.019495  0.062764  0.0  0.0  0.0  0.0  0.577269\n",
       "1  4136.0  0.026908  0.079235  0.0  0.0  0.0  0.0  0.629499\n",
       "2  4136.0  0.000029  0.001796  0.0  0.0  0.0  0.0  0.115453\n",
       "3  4136.0  0.016227  0.052464  0.0  0.0  0.0  0.0  0.598774\n",
       "4  4136.0  0.001174  0.017275  0.0  0.0  0.0  0.0  0.599406"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe().T.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 10s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = DecisionTreeID3(max_depth=5, min_samples_split=2)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241545893719807"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592592592592593"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711627094008445"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
